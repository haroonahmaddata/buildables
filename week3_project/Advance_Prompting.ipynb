{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DYeTIZ3Lewz"
      },
      "source": [
        "#Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6cJxQjKqHsjC"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNWAlEFwN_6_"
      },
      "source": [
        "#Setting up Gemini for prompt processing and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KNyg8_b0HySO"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Replace 'YOUR_API_KEY' with your actual API key\n",
        "GEMINI_API_KEY = ''\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "def get_gemini_response(prompt):\n",
        "    \"\"\"\n",
        "    Sends a prompt to the Gemini model and returns the generated text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        # The text is in the 'text' attribute of the first part of the response\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiSuqbwcEfIw"
      },
      "source": [
        "# Loading Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCycS-lfxpeD",
        "outputId": "c259e66f-eb85-4e72-d738-8fe3afc7603f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First logic puzzle: \n",
            " {'id': 1, 'puzzle': 'Alice is older than Bob. Bob is older than Charlie. Who is the youngest?', 'expected_answer': 'Charlie'}\n",
            "First math problem: \n",
            " {'id': 1, 'problem': 'A train travels 60 km in 1 hour. How far will it go in 4 hours?', 'expected_answer': '240 km'}\n",
            "First reasoning task: \n",
            " {'id': 1, 'task': 'If John is taller than Mary, and Mary is taller than Sam, who is the tallest?', 'expected_answer': 'John'}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Load each dataset\n",
        "with open('logic-puzzles.json', 'r') as f:\n",
        "    logic_puzzles = json.load(f)\n",
        "\n",
        "with open('math-problems.json', 'r') as f:\n",
        "    math_problems = json.load(f)\n",
        "\n",
        "with open('reasoning-tasks.json', 'r') as f:\n",
        "    reasoning_tasks = json.load(f)\n",
        "\n",
        "print(\"First logic puzzle:\", \"\\n\", logic_puzzles[0])\n",
        "print(\"First math problem:\", \"\\n\", math_problems[0])\n",
        "print(\"First reasoning task:\", \"\\n\",reasoning_tasks[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc5POYhLZD5v"
      },
      "source": [
        "#Selecting Test data from Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5hj8Uf4FVLS",
        "outputId": "6de754a9-303e-4409-e13a-17cf2d8086b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 2, 'task': 'A farmer has 5 cows, each produces 8 liters of milk per day. How much milk in total per day?', 'expected_answer': '40 liters'}\n"
          ]
        }
      ],
      "source": [
        "selected_logic_puzzle = logic_puzzles[0]\n",
        "selected_math_problem = math_problems[0]\n",
        "selected_reasoning_task = reasoning_tasks[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ6jVl-PZNoA"
      },
      "source": [
        "# Zero Shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "CadREdnTDoj4"
      },
      "outputs": [],
      "source": [
        "zero_shot_logic = selected_logic_puzzle['puzzle']\n",
        "zero_shot_math = selected_math_problem['problem']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ8FVFGWZTE3"
      },
      "source": [
        "# Few Shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPQ4jZMbGbiR",
        "outputId": "d0465f16-eae3-45f3-834e-5c54d1641588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Few-Shot Prompts ---\n",
            "Logic Prompt:\n",
            "You are expert in solving logic puzzles, your task is to solve the following puzzles.\n",
            "\n",
            "Q: Who is the youngest if A is older than B, and B is older than C?\n",
            "A: C is the youngest.\n",
            "\n",
            "Q: Who is the youngest if David is older than Emily, and Emily is older than Frank?\n",
            "A: Frank is the youngest.\n",
            "\n",
            "Q: Who is the youngest if Jane is older than Mike, and Mike is older than Chris?\n",
            "A: Chris is the youngest.\n",
            "\n",
            "Q: Alice is older than Bob. Bob is older than Charlie. Who is the youngest?\n",
            "A:\n",
            "\n",
            "Math Prompt:\n",
            "You are expert mathematician, your task is to solve the following questions.\n",
            "\n",
            "Q: A car travels 50 km in 1 hour. How far will it go in 3 hours?\n",
            "A: 150 km.\n",
            "\n",
            "Q: A runner's average speed is 10 km per hour. How far will they run in 2 hours?\n",
            "A: 20 km.\n",
            "\n",
            "Q: A cyclist travels at a constant speed of 25 km/h. How far will they travel in 4 hours?\n",
            "A: 100 km.\n",
            "\n",
            "Q: A train travels 60 km in 1 hour. How far will it go in 4 hours?\n",
            "A:\n"
          ]
        }
      ],
      "source": [
        "# ew-shot prompt for the Logic task with 3 examples\n",
        "few_shot_logic = (\n",
        "    \"You are expert in solving logic puzzles, your task is to solve the following puzzles.\\n\\n\"\n",
        "    \"Q: Who is the youngest if A is older than B, and B is older than C?\\n\"\n",
        "    \"A: C is the youngest.\\n\\n\"\n",
        "    \"Q: Who is the youngest if David is older than Emily, and Emily is older than Frank?\\n\"\n",
        "    \"A: Frank is the youngest.\\n\\n\"\n",
        "    \"Q: Who is the youngest if Jane is older than Mike, and Mike is older than Chris?\\n\"\n",
        "    \"A: Chris is the youngest.\\n\\n\"\n",
        "    f\"Q: {selected_logic_puzzle['puzzle']}\\n\"\n",
        "    \"A:\"\n",
        ")\n",
        "\n",
        "# Improved few-shot prompt for the Math task with 3 examples\n",
        "few_shot_math = (\n",
        "    \"You are expert mathematician, your task is to solve the following questions.\\n\\n\"\n",
        "    \"Q: A car travels 50 km in 1 hour. How far will it go in 3 hours?\\n\"\n",
        "    \"A: 150 km.\\n\\n\"\n",
        "    \"Q: A runner's average speed is 10 km per hour. How far will they run in 2 hours?\\n\"\n",
        "    \"A: 20 km.\\n\\n\"\n",
        "    \"Q: A cyclist travels at a constant speed of 25 km/h. How far will they travel in 4 hours?\\n\"\n",
        "    \"A: 100 km.\\n\\n\"\n",
        "    f\"Q: {selected_math_problem['problem']}\\n\"\n",
        "    \"A:\"\n",
        ")\n",
        "\n",
        "# Example of how you would use these variables\n",
        "print(\"--- Few-Shot Prompts ---\")\n",
        "print(\"Logic Prompt:\")\n",
        "print(few_shot_logic)\n",
        "print(\"\\nMath Prompt:\")\n",
        "print(few_shot_math)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHxNVCoTZYJY"
      },
      "source": [
        "#Chain of thought Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "m9oOLXWlHd7V"
      },
      "outputs": [],
      "source": [
        "cot_logic = f\"{selected_logic_puzzle['puzzle']} Let's think step by step.\"\n",
        "cot_math = f\"{selected_math_problem['problem']} Explain your reasoning.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KKpfoAwZdEs"
      },
      "source": [
        "#Text generation using Gemini 2.0 Flash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Iq0KZw0zHh-s"
      },
      "outputs": [],
      "source": [
        "def get_gemini_response(prompt):\n",
        "    \"\"\"\n",
        "    Sends a prompt to the Gemini model and returns the generated text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        # The text is in the 'text' attribute of the first part of the response\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4L6s8uCZmNJ"
      },
      "source": [
        "# Returning all model outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "cp_VaIE4IAMP",
        "outputId": "1cbb6d65-82b4-482e-b4f0-9a1cada18ffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Logic Puzzle prompts...\n",
            "\n",
            "Running Math Problem prompts...\n",
            "\n",
            "All prompts have been run. Outputs are stored in the 'model_outputs' dictionary.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# A dictionary to store all the model's responses\n",
        "model_outputs = {}\n",
        "\n",
        "print(\"Running Logic Puzzle prompts...\")\n",
        "model_outputs['logic_zero_shot'] = get_gemini_response(zero_shot_logic)\n",
        "model_outputs['logic_few_shot'] = get_gemini_response(few_shot_logic)\n",
        "model_outputs['logic_cot'] = get_gemini_response(cot_logic)\n",
        "\n",
        "print(\"\\nRunning Math Problem prompts...\")\n",
        "model_outputs['math_zero_shot'] = get_gemini_response(zero_shot_math)\n",
        "model_outputs['math_few_shot'] = get_gemini_response(few_shot_math)\n",
        "model_outputs['math_cot'] = get_gemini_response(cot_math)\n",
        "\n",
        "print(\"\\nAll prompts have been run. Outputs are stored in the 'model_outputs' dictionary.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmnA8bUxZrIY"
      },
      "source": [
        "# Printing Model Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ztg8tYAIEjH",
        "outputId": "4a54798e-7e02-4ac6-a257-e12eef6f0d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- LOGIC ZERO SHOT ---\n",
            "Charlie is the youngest.\n",
            "\n",
            "------------------------------\n",
            "--- LOGIC FEW SHOT ---\n",
            "Charlie is the youngest.\n",
            "\n",
            "------------------------------\n",
            "--- LOGIC COT ---\n",
            "1. **Alice is older than Bob:** This tells us Bob is younger than Alice.\n",
            "2. **Bob is older than Charlie:** This tells us Charlie is younger than Bob.\n",
            "\n",
            "Since Charlie is younger than Bob, and Bob is younger than Alice, Charlie must be the youngest.\n",
            "\n",
            "**Answer:** Charlie is the youngest.\n",
            "\n",
            "------------------------------\n",
            "--- MATH ZERO SHOT ---\n",
            "The train will travel 240 km in 4 hours.\n",
            "\n",
            "**Explanation**\n",
            "\n",
            "* **Distance = Speed x Time**\n",
            "\n",
            "* **Speed:** The train travels 60 km in 1 hour, so its speed is 60 km/hour.\n",
            "* **Time:**  The time is 4 hours.\n",
            "\n",
            "* **Distance = 60 km/hour * 4 hours = 240 km**\n",
            "\n",
            "------------------------------\n",
            "--- MATH FEW SHOT ---\n",
            "240 km.\n",
            "\n",
            "------------------------------\n",
            "--- MATH COT ---\n",
            "If the train travels 60 km in 1 hour, then in 4 hours it will travel four times that distance.\n",
            "\n",
            "So, 60 km/hour * 4 hours = 240 km\n",
            "\n",
            "The train will travel **240 km** in 4 hours.\n",
            "\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Print out the results for easy viewing\n",
        "for key, value in model_outputs.items():\n",
        "    print(f\"--- {key.replace('_', ' ').upper()} ---\")\n",
        "    print(value)\n",
        "    print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-h-8GkZu9f"
      },
      "source": [
        "#Evaluating Model output using specified Evaluation rubric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUrJ4wzJLAVT",
        "outputId": "28315df6-c72e-4394-eb5d-b3d199867616"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reasoning Evaluation Results (DataFrame)\n",
            "\n",
            "    Task Prompt Type  Correctness  Clarity  Completeness  Conciseness  Total Score\n",
            "0  Logic   Zero-Shot            3        0             1            3            7\n",
            "1  Logic    Few-Shot            3        0             1            3            7\n",
            "2  Logic         CoT            3        3             3            3           12\n",
            "3   Math   Zero-Shot            3        3             3            3           12\n",
            "4   Math    Few-Shot            3        0             1            3            7\n",
            "5   Math         CoT            3        2             2            2            9\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def evaluate_response(prompt_type: str, model_output: str) -> dict:\n",
        "    \"\"\"\n",
        "    Evaluates a model's response based on the provided Reasoning Evaluation Rubric.\n",
        "    \"\"\"\n",
        "    scores = {\n",
        "        \"correctness\": 0,\n",
        "        \"reasoning_clarity\": 0,\n",
        "        \"completeness\": 0,\n",
        "        \"conciseness\": 0,\n",
        "        \"total_score\": 0\n",
        "    }\n",
        "\n",
        "    # Correctness is assumed to be fully correct for all provided outputs.\n",
        "    scores[\"correctness\"] = 3\n",
        "\n",
        "    # --- Reasoning Clarity & Completeness ---\n",
        "    # These scores are based on the presence of a step-by-step breakdown.\n",
        "    # We look for common indicators like numbered lists, bullet points, or the word \"explanation\".\n",
        "    has_detailed_reasoning = (\n",
        "        \"1. **\" in model_output or\n",
        "        \"Explanation\" in model_output or\n",
        "        \"step by step\" in model_output.lower() or\n",
        "        \"Let's think\" in model_output.lower()\n",
        "    )\n",
        "\n",
        "    if has_detailed_reasoning:\n",
        "        scores[\"reasoning_clarity\"] = 3\n",
        "        scores[\"completeness\"] = 3\n",
        "    elif prompt_type.lower() == \"cot\":\n",
        "        # This is a fallback for CoT if the formatting is not caught,\n",
        "        # but the prompt type suggests a detailed response was intended.\n",
        "        scores[\"reasoning_clarity\"] = 2\n",
        "        scores[\"completeness\"] = 2\n",
        "    else:\n",
        "        # Simple answers from zero-shot or few-shot have low clarity/completeness.\n",
        "        scores[\"reasoning_clarity\"] = 0\n",
        "        scores[\"completeness\"] = 1\n",
        "\n",
        "    # --- Conciseness ---\n",
        "    # A concise score depends on the detail level.\n",
        "    if has_detailed_reasoning:\n",
        "        # A detailed answer should not be overly verbose, but is inherently longer.\n",
        "        # We assume well-structured reasoning (like in the examples) is concise for its type.\n",
        "        scores[\"conciseness\"] = 3\n",
        "    elif len(model_output.strip().split()) <= 4:\n",
        "        # Very short answers from few-shot are highly concise.\n",
        "        scores[\"conciseness\"] = 3\n",
        "    else:\n",
        "        # Answers that are a full sentence but not step-by-step are somewhat concise.\n",
        "        scores[\"conciseness\"] = 2\n",
        "\n",
        "    scores[\"total_score\"] = sum([\n",
        "        scores[\"correctness\"],\n",
        "        scores[\"reasoning_clarity\"],\n",
        "        scores[\"completeness\"],\n",
        "        scores[\"conciseness\"]\n",
        "    ])\n",
        "\n",
        "    return scores\n",
        "\n",
        "# --- Corrected Data to be evaluated ---\n",
        "test_cases = [\n",
        "    {\n",
        "        \"task\": \"Logic\",\n",
        "        \"prompt_type\": \"Zero-Shot\",\n",
        "        \"output\": \"Charlie is the youngest.\"\n",
        "    },\n",
        "    {\n",
        "        \"task\": \"Logic\",\n",
        "        \"prompt_type\": \"Few-Shot\",\n",
        "        \"output\": \"Charlie is the youngest.\"\n",
        "    },\n",
        "    {\n",
        "        \"task\": \"Logic\",\n",
        "        \"prompt_type\": \"CoT\",\n",
        "        \"output\": \"1. **Alice is older than Bob:** This tells us Bob is younger than Alice.\\n2. **Bob is older than Charlie:** This tells us Charlie is younger than Bob.\\n\\nSince Charlie is younger than Bob, and Bob is younger than Alice, Charlie must be the youngest.\\n\\n**Answer:** Charlie is the youngest.\"\n",
        "    },\n",
        "    {\n",
        "        \"task\": \"Math\",\n",
        "        \"prompt_type\": \"Zero-Shot\",\n",
        "        \"output\": \"The train will travel 240 km in 4 hours.\\n\\n**Explanation**\\n\\n* **Distance = Speed x Time**\\n\\n* **Speed:** The train travels 60 km in 1 hour, so its speed is 60 km/hour.\\n* **Time:** The time is 4 hours.\\n\\n* **Distance = 60 km/hour * 4 hours = 240 km**\"\n",
        "    },\n",
        "    {\n",
        "        \"task\": \"Math\",\n",
        "        \"prompt_type\": \"Few-Shot\",\n",
        "        \"output\": \"240 km.\"\n",
        "    },\n",
        "    {\n",
        "        \"task\": \"Math\",\n",
        "        \"prompt_type\": \"CoT\",\n",
        "        \"output\": \"If the train travels 60 km in 1 hour, then in 4 hours it will travel four times that distance.\\n\\nSo, 60 km/hour * 4 hours = 240 km\\n\\nThe train will travel **240 km** in 4 hours.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Run the evaluation and create the DataFrame ---\n",
        "evaluation_data = []\n",
        "for case in test_cases:\n",
        "    scores = evaluate_response(case[\"prompt_type\"], case[\"output\"])\n",
        "    evaluation_data.append({\n",
        "        \"Task\": case[\"task\"],\n",
        "        \"Prompt Type\": case[\"prompt_type\"],\n",
        "        \"Correctness\": scores[\"correctness\"],\n",
        "        \"Clarity\": scores[\"reasoning_clarity\"],\n",
        "        \"Completeness\": scores[\"completeness\"],\n",
        "        \"Conciseness\": scores[\"conciseness\"],\n",
        "        \"Total Score\": scores[\"total_score\"]\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(evaluation_data)\n",
        "\n",
        "# --- Print the DataFrame ---\n",
        "print(\"Reasoning Evaluation Results (DataFrame)\\n\")\n",
        "print(df.to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqWoiIrgYEaI"
      },
      "source": [
        "### Justification of Evaluation Results\n",
        "\n",
        "The evaluation results are based on a rubric that scores four key criteria: Correctness, Clarity, Completeness, and Conciseness.\n",
        "\n",
        "*   **Correctness (Score: 3)**: All responses are correct, so they all receive a perfect score.\n",
        "    \n",
        "*   **Clarity (Score: 0, 2, or 3)**: This score reflects how well the reasoning is explained.\n",
        "    \n",
        "    *   **Score 0**: Given for Zero-Shot and Few-Shot prompts that provide only the final answer without any explanation.\n",
        "        \n",
        "    *   **Score 2**: Given for the Math CoT output, which offers a good but not perfectly structured explanation.\n",
        "        \n",
        "    *   **Score 3**: Given for Logic CoT and Math Zero-Shot, which provide detailed, step-by-step breakdowns of the solution.\n",
        "        \n",
        "*   **Completeness (Score: 1, 2, or 3)**: This measures if the full solution is explained.\n",
        "    \n",
        "    *   **Score 1**: Given for Zero-Shot and Few-Shot outputs, which are incomplete as they only provide the final answer.\n",
        "        \n",
        "    *   **Score 2**: Given for the Math CoT output, which presents core reasoning but lacks a full breakdown of the problem.\n",
        "        \n",
        "    *   **Score 3**: Given for Logic CoT and Math Zero-Shot, which provide a comprehensive solution with full reasoning and steps.\n",
        "        \n",
        "*   **Conciseness (Score: 2 or 3)**: This score measures how brief and to the point the response is.\n",
        "    \n",
        "    *   **Score 3**: Given for all responses except the Math CoT. The brief answers are perfectly concise, while the detailed answers are highly efficient for their purpose.\n",
        "        \n",
        "    *   **Score 2**: Given for the Math CoT output, as its explanation is slightly conversational and less structured.\n",
        "        \n",
        "\n",
        "### Summary of Total Scores\n",
        "\n",
        "*   **Zero-Shot/Few-Shot** responses score a **7** due to low scores in Clarity and Completeness.\n",
        "    \n",
        "*   **Logic CoT** and **Math Zero-Shot** achieve a perfect **12**, showing that a correct, well-structured explanation earns high marks across all categories.\n",
        "    \n",
        "*   **Math CoT** scores a **9**, reflecting a solid but less structured explanation compared to the top-scoring outputs.\n",
        "    \n",
        "\n",
        "This justification shows that the highest-rated responses are those that are not only correct but also provide a clear, complete, and concise explanation of their reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HutiGx6DLMWe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
